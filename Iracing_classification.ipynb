{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TomazFilgueira/UFRN-ML-2025-1-AutoParts_Classification/blob/main/Iracing_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "y8oXM6PjM9nt"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries for randomness, deep copying, and numerical operations\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Import libraries for image processing and data manipulation\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Import PyTorch core and utilities for deep learning\n",
    "import torch\n",
    "import torch.optim as optim  # Optimization algorithms\n",
    "import torch.nn as nn  # Neural network modules\n",
    "import torch.nn.functional as F  # Functional API for non-parametric operations\n",
    "\n",
    "# Import PyTorch utilities for data loading and transformations\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
    "from torchvision.transforms.v2 import Compose, ToImage, Normalize, ToPILImage, Resize, ToDtype\n",
    "\n",
    "# Import dataset handling and learning rate schedulers\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, MultiStepLR, CyclicLR, LambdaLR\n",
    "\n",
    "# Import visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set matplotlib style for better visuals\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8snON0VfNXTg"
   },
   "outputs": [],
   "source": [
    "class Architecture(object):\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        # Here we define the attributes of our class\n",
    "\n",
    "        # We start by storing the arguments as attributes\n",
    "        # to use them later\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # Let's send the model to the specified device right away\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # These attributes are defined here, but since they are\n",
    "        # not informed at the moment of creation, we keep them None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "\n",
    "        # These attributes are going to be computed internally\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.total_epochs = 0\n",
    "\n",
    "        # Creates the train_step function for our model,\n",
    "        # loss function and optimizer\n",
    "        # Note: there are NO ARGS there! It makes use of the class\n",
    "        # attributes directly\n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        # Creates the val_step function for our model and loss\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "\n",
    "        # for hook purposes\n",
    "        self.handles = {}\n",
    "        self.visualization = {}\n",
    "\n",
    "    def to(self, device):\n",
    "        # This method allows the user to specify a different device\n",
    "        # It sets the corresponding attribute (to be used later in\n",
    "        # the mini-batches) and sends the model to the device\n",
    "        try:\n",
    "            self.device = device\n",
    "            self.model.to(self.device)\n",
    "        except RuntimeError:\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(f\"Couldn't send it to {device}, sending it to {self.device} instead.\")\n",
    "            self.model.to(self.device)\n",
    "\n",
    "    def set_loaders(self, train_loader, val_loader=None):\n",
    "        # This method allows the user to define which train_loader (and val_loader, optionally) to use\n",
    "        # Both loaders are then assigned to attributes of the class\n",
    "        # So they can be referred to later\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "    def _make_train_step_fn(self):\n",
    "        # This method does not need ARGS... it can refer to\n",
    "        # the attributes: self.model, self.loss_fn and self.optimizer\n",
    "\n",
    "        # Builds function that performs a step in the train loop\n",
    "        def perform_train_step_fn(x, y):\n",
    "            # Sets model to TRAIN mode\n",
    "            self.model.train()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # Step 3 - Computes gradients for both \"a\" and \"b\" parameters\n",
    "            loss.backward()\n",
    "            # Step 4 - Updates parameters using gradients and the learning rate\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Returns the loss\n",
    "            return loss.item()\n",
    "\n",
    "        # Returns the function that will be called inside the train loop\n",
    "        return perform_train_step_fn\n",
    "\n",
    "    def _make_val_step_fn(self):\n",
    "        # Builds function that performs a step in the validation loop\n",
    "        def perform_val_step_fn(x, y):\n",
    "            # Sets model to EVAL mode\n",
    "            self.model.eval()\n",
    "\n",
    "            # Step 1 - Computes our model's predicted output - forward pass\n",
    "            yhat = self.model(x)\n",
    "            # Step 2 - Computes the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # There is no need to compute Steps 3 and 4, since we don't update parameters during evaluation\n",
    "            return loss.item()\n",
    "\n",
    "        return perform_val_step_fn\n",
    "\n",
    "    def _mini_batch(self, validation=False):\n",
    "        # The mini-batch can be used with both loaders\n",
    "        # The argument `validation`defines which loader and\n",
    "        # corresponding step function is going to be used\n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "        else:\n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "\n",
    "        # Once the data loader and step function, this is the same\n",
    "        # mini-batch loop we had before\n",
    "        mini_batch_losses = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "\n",
    "            mini_batch_loss = step_fn(x_batch, y_batch)\n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "\n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        return loss\n",
    "\n",
    "    # this function was updated in this class\n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        try:\n",
    "            self.train_loader.sampler.generator.manual_seed(seed)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "    def train(self, n_epochs, seed=42):\n",
    "        # To ensure reproducibility of the training process\n",
    "        self.set_seed(seed)\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            # Keeps track of the numbers of epochs\n",
    "            # by updating the corresponding attribute\n",
    "            self.total_epochs += 1\n",
    "\n",
    "            # inner loop\n",
    "            # Performs training using mini-batches\n",
    "            loss = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            # VALIDATION\n",
    "            # no gradients in validation!\n",
    "            with torch.no_grad():\n",
    "                # Performs evaluation using mini-batches\n",
    "                val_loss = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        # Builds dictionary with all elements for resuming training\n",
    "        checkpoint = {'epoch': self.total_epochs,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'val_loss': self.val_losses}\n",
    "\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename):\n",
    "        # Loads dictionary\n",
    "        checkpoint = torch.load(filename)\n",
    "\n",
    "        # Restore state for model and optimizer\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        self.total_epochs = checkpoint['epoch']\n",
    "        self.losses = checkpoint['loss']\n",
    "        self.val_losses = checkpoint['val_loss']\n",
    "\n",
    "        self.model.train() # always use TRAIN for resuming training\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Set is to evaluation mode for predictions\n",
    "        self.model.eval()\n",
    "        # Takes aNumpy input and make it a float tensor\n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        # Send input to device and uses model for prediction\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        # Set it back to train mode\n",
    "        self.model.train()\n",
    "        # Detaches it, brings it to CPU and back to Numpy\n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "\n",
    "    def count_parameters(self):\n",
    "      return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "\n",
    "    def plot_losses(self):\n",
    "        fig = plt.figure(figsize=(10, 4))\n",
    "        plt.plot(self.losses, label='Training Loss', c='b')\n",
    "        plt.plot(self.val_losses, label='Validation Loss', c='r')\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    @staticmethod\n",
    "    def _visualize_tensors(axs, x, y=None, yhat=None, layer_name='', title=None):\n",
    "        # The number of images is the number of subplots in a row\n",
    "        n_images = len(axs)\n",
    "        # Gets max and min values for scaling the grayscale\n",
    "        minv, maxv = np.min(x[:n_images]), np.max(x[:n_images])\n",
    "        # For each image\n",
    "        for j, image in enumerate(x[:n_images]):\n",
    "            ax = axs[j]\n",
    "            # Sets title, labels, and removes ticks\n",
    "            if title is not None:\n",
    "                ax.set_title(f'{title} #{j}', fontsize=12)\n",
    "            shp = np.atleast_2d(image).shape\n",
    "            ax.set_ylabel(\n",
    "                f'{layer_name}\\n{shp[0]}x{shp[1]}',\n",
    "                rotation=0, labelpad=40\n",
    "            )\n",
    "            xlabel1 = '' if y is None else f'\\nLabel: {y[j]}'\n",
    "            xlabel2 = '' if yhat is None else f'\\nPredicted: {yhat[j]}'\n",
    "            xlabel = f'{xlabel1}{xlabel2}'\n",
    "            if len(xlabel):\n",
    "                ax.set_xlabel(xlabel, fontsize=12)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "            # Plots weight as an image\n",
    "            ax.imshow(\n",
    "                np.atleast_2d(image.squeeze()),\n",
    "                cmap='gray',\n",
    "                vmin=minv,\n",
    "                vmax=maxv\n",
    "            )\n",
    "        return\n",
    "\n",
    "    def visualize_filters(self, layer_name, **kwargs):\n",
    "        try:\n",
    "            # Gets the layer object from the model\n",
    "            layer = self.model\n",
    "            for name in layer_name.split('.'):\n",
    "                layer = getattr(layer, name)\n",
    "            # We are only looking at filters for 2D convolutions\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                # Takes the weight information\n",
    "                weights = layer.weight.data.cpu().numpy()\n",
    "                # weights -> (channels_out (filter), channels_in, H, W)\n",
    "                n_filters, n_channels, _, _ = weights.shape\n",
    "\n",
    "                # Builds a figure\n",
    "                size = (2 * n_channels + 2, 2 * n_filters)\n",
    "                fig, axes = plt.subplots(n_filters, n_channels,\n",
    "                                        figsize=size)\n",
    "                axes = np.atleast_2d(axes)\n",
    "                axes = axes.reshape(n_filters, n_channels)\n",
    "                # For each channel_out (filter)\n",
    "                for i in range(n_filters):\n",
    "                    Architecture._visualize_tensors(\n",
    "                        axes[i, :],\n",
    "                        weights[i],\n",
    "                        layer_name=f'Filter #{i}',\n",
    "                        title='Channel'\n",
    "                    )\n",
    "\n",
    "                for ax in axes.flat:\n",
    "                    ax.label_outer()\n",
    "\n",
    "                fig.tight_layout()\n",
    "                return fig\n",
    "        except AttributeError:\n",
    "            return\n",
    "\n",
    "    def attach_hooks(self, layers_to_hook, hook_fn=None):\n",
    "        # Clear any previous values\n",
    "        self.visualization = {}\n",
    "        # Creates the dictionary to map layer objects to their names\n",
    "        modules = list(self.model.named_modules())\n",
    "        layer_names = {layer: name for name, layer in modules[1:]}\n",
    "\n",
    "        if hook_fn is None:\n",
    "            # Hook function to be attached to the forward pass\n",
    "            def hook_fn(layer, inputs, outputs):\n",
    "                # Gets the layer name\n",
    "                name = layer_names[layer]\n",
    "                # Detaches outputs\n",
    "                values = outputs.detach().cpu().numpy()\n",
    "                # Since the hook function may be called multiple times\n",
    "                # for example, if we make predictions for multiple mini-batches\n",
    "                # it concatenates the results\n",
    "                if self.visualization[name] is None:\n",
    "                    self.visualization[name] = values\n",
    "                else:\n",
    "                    self.visualization[name] = np.concatenate([self.visualization[name], values])\n",
    "\n",
    "        for name, layer in modules:\n",
    "            # If the layer is in our list\n",
    "            if name in layers_to_hook:\n",
    "                # Initializes the corresponding key in the dictionary\n",
    "                self.visualization[name] = None\n",
    "                # Register the forward hook and keep the handle in another dict\n",
    "                self.handles[name] = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        # Loops through all hooks and removes them\n",
    "        for handle in self.handles.values():\n",
    "            handle.remove()\n",
    "        # Clear the dict, as all hooks have been removed\n",
    "        self.handles = {}\n",
    "\n",
    "    def visualize_outputs(self, layers, n_images=10, y=None, yhat=None):\n",
    "        layers = filter(lambda l: l in self.visualization.keys(), layers)\n",
    "        layers = list(layers)\n",
    "        shapes = [self.visualization[layer].shape for layer in layers]\n",
    "        n_rows = [shape[1] if len(shape) == 4 else 1\n",
    "                  for shape in shapes]\n",
    "        total_rows = np.sum(n_rows)\n",
    "\n",
    "        fig, axes = plt.subplots(total_rows, n_images,\n",
    "                                figsize=(1.5*n_images, 1.5*total_rows))\n",
    "        axes = np.atleast_2d(axes).reshape(total_rows, n_images)\n",
    "\n",
    "        # Loops through the layers, one layer per row of subplots\n",
    "        row = 0\n",
    "        for i, layer in enumerate(layers):\n",
    "            start_row = row\n",
    "            # Takes the produced feature maps for that layer\n",
    "            output = self.visualization[layer]\n",
    "\n",
    "            is_vector = len(output.shape) == 2\n",
    "\n",
    "            for j in range(n_rows[i]):\n",
    "                Architecture._visualize_tensors(\n",
    "                    axes[row, :],\n",
    "                    output if is_vector else output[:, j].squeeze(),\n",
    "                    y,\n",
    "                    yhat,\n",
    "                    layer_name=layers[i] \\\n",
    "                              if is_vector \\\n",
    "                              else f'{layers[i]}\\nfil#{row-start_row}',\n",
    "                    title='Image' if (row == 0) else None\n",
    "                )\n",
    "                row += 1\n",
    "\n",
    "        for ax in axes.flat:\n",
    "            ax.label_outer()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def correct(self, x, y, threshold=.5):\n",
    "        self.model.eval()\n",
    "        yhat = self.model(x.to(self.device))\n",
    "        y = y.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        # We get the size of the batch and the number of classes\n",
    "        # (only 1, if it is binary)\n",
    "        n_samples, n_dims = yhat.shape\n",
    "        if n_dims > 1:\n",
    "            # In a multiclass classification, the biggest logit\n",
    "            # always wins, so we don't bother getting probabilities\n",
    "\n",
    "            # This is PyTorch's version of argmax,\n",
    "            # but it returns a tuple: (max value, index of max value)\n",
    "            _, predicted = torch.max(yhat, 1)\n",
    "        else:\n",
    "            n_dims += 1\n",
    "            # In binary classification, we NEED to check if the\n",
    "            # last layer is a sigmoid (and then it produces probs)\n",
    "            if isinstance(self.model, nn.Sequential) and \\\n",
    "              isinstance(self.model[-1], nn.Sigmoid):\n",
    "                predicted = (yhat > threshold).long()\n",
    "            # or something else (logits), which we need to convert\n",
    "            # using a sigmoid\n",
    "            else:\n",
    "                predicted = (F.sigmoid(yhat) > threshold).long()\n",
    "\n",
    "        # How many samples got classified correctly for each class\n",
    "        result = []\n",
    "        for c in range(n_dims):\n",
    "            n_class = (y == c).sum().item()\n",
    "            n_correct = (predicted[y == c] == c).sum().item()\n",
    "            result.append((n_correct, n_class))\n",
    "        return torch.tensor(result)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def loader_apply(loader, func, reduce='sum'):\n",
    "        results = [func(x, y) for i, (x, y) in enumerate(loader)]\n",
    "        results = torch.stack(results, axis=0)\n",
    "\n",
    "        if reduce == 'sum':\n",
    "            results = results.sum(axis=0)\n",
    "        elif reduce == 'mean':\n",
    "            results = results.float().mean(axis=0)\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def statistics_per_channel(images, labels):\n",
    "        # NCHW\n",
    "        n_samples, n_channels, n_height, n_weight = images.size()\n",
    "        # Flatten HW into a single dimension\n",
    "        flatten_per_channel = images.reshape(n_samples, n_channels, -1)\n",
    "\n",
    "        # Computes statistics of each image per channel\n",
    "        # Average pixel value per channel\n",
    "        # (n_samples, n_channels)\n",
    "        means = flatten_per_channel.mean(axis=2)\n",
    "        # Standard deviation of pixel values per channel\n",
    "        # (n_samples, n_channels)\n",
    "        stds = flatten_per_channel.std(axis=2)\n",
    "\n",
    "        # Adds up statistics of all images in a mini-batch\n",
    "        # (1, n_channels)\n",
    "        sum_means = means.sum(axis=0)\n",
    "        sum_stds = stds.sum(axis=0)\n",
    "        # Makes a tensor of shape (1, n_channels)\n",
    "        # with the number of samples in the mini-batch\n",
    "        n_samples = torch.tensor([n_samples]*n_channels).float()\n",
    "\n",
    "        # Stack the three tensors on top of one another\n",
    "        # (3, n_channels)\n",
    "        return torch.stack([n_samples, sum_means, sum_stds], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_normalizer(loader):\n",
    "        total_samples, total_means, total_stds = Architecture.loader_apply(loader, Architecture.statistics_per_channel)\n",
    "        norm_mean = total_means / total_samples\n",
    "        norm_std = total_stds / total_samples\n",
    "        return Normalize(mean=norm_mean, std=norm_std)\n",
    "\n",
    "    def lr_range_test(self, data_loader, end_lr, num_iter=100, step_mode='exp', alpha=0.05, ax=None):\n",
    "        # Since the test updates both model and optimizer we need to store\n",
    "        # their initial states to restore them in the end\n",
    "        previous_states = {'model': deepcopy(self.model.state_dict()),\n",
    "                          'optimizer': deepcopy(self.optimizer.state_dict())}\n",
    "        # Retrieves the learning rate set in the optimizer\n",
    "        start_lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "        # Builds a custom function and corresponding scheduler\n",
    "        lr_fn = make_lr_fn(start_lr, end_lr, num_iter)\n",
    "        scheduler = LambdaLR(self.optimizer, lr_lambda=lr_fn)\n",
    "\n",
    "        # Variables for tracking results and iterations\n",
    "        tracking = {'loss': [], 'lr': []}\n",
    "        iteration = 0\n",
    "\n",
    "        # If there are more iterations than mini-batches in the data loader,\n",
    "        # it will have to loop over it more than once\n",
    "        while (iteration < num_iter):\n",
    "            # That's the typical mini-batch inner loop\n",
    "            for x_batch, y_batch in data_loader:\n",
    "                x_batch = x_batch.to(self.device)\n",
    "                y_batch = y_batch.to(self.device)\n",
    "                # Step 1\n",
    "                yhat = self.model(x_batch)\n",
    "                # Step 2\n",
    "                loss = self.loss_fn(yhat, y_batch)\n",
    "                # Step 3\n",
    "                loss.backward()\n",
    "\n",
    "                # Here we keep track of the losses (smoothed)\n",
    "                # and the learning rates\n",
    "                tracking['lr'].append(scheduler.get_last_lr()[0])\n",
    "                if iteration == 0:\n",
    "                    tracking['loss'].append(loss.item())\n",
    "                else:\n",
    "                    prev_loss = tracking['loss'][-1]\n",
    "                    smoothed_loss = alpha * loss.item() + (1-alpha) * prev_loss\n",
    "                    tracking['loss'].append(smoothed_loss)\n",
    "\n",
    "                iteration += 1\n",
    "                # Number of iterations reached\n",
    "                if iteration == num_iter:\n",
    "                    break\n",
    "\n",
    "                # Step 4\n",
    "                self.optimizer.step()\n",
    "                scheduler.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "        # Restores the original states\n",
    "        self.optimizer.load_state_dict(previous_states['optimizer'])\n",
    "        self.model.load_state_dict(previous_states['model'])\n",
    "\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "        else:\n",
    "            fig = ax.get_figure()\n",
    "        ax.plot(tracking['lr'], tracking['loss'])\n",
    "        if step_mode == 'exp':\n",
    "            ax.set_xscale('log')\n",
    "        ax.set_xlabel('Learning Rate')\n",
    "        ax.set_ylabel('Loss')\n",
    "        fig.tight_layout()\n",
    "        return tracking, fig\n",
    "\n",
    "    def set_optimizer(self, optimizer):\n",
    "        self.optimizer = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self, n_feature, p=0.0):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.p = p\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        # Creates the linear layers\n",
    "        # Where do this 5 * 5 come from?! Check it below\n",
    "        self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, 4)  # Changed from 3 to 4 classes\n",
    "        # Creates dropout layers\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # Featurizer\n",
    "        # First convolutional block\n",
    "        # 3@28x28 -> n_feature@26x26 -> n_feature@13x13\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        # Second convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        # Input dimension (n_feature@5x5)\n",
    "        # Output dimension (n_feature * 5 * 5)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        # Classifier\n",
    "        # Hidden Layer\n",
    "        # Input dimension (n_feature * 5 * 5)\n",
    "        # Output dimension (50)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # Output Layer\n",
    "        # Input dimension (50)\n",
    "        # Output dimension (4)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOaHaWUUNdJq"
   },
   "source": [
    "# Import Data folder\n",
    "\n",
    "dataset_iracing/\n",
    "\n",
    " - reta\n",
    " - freada_apex\n",
    " - saida_curva\n",
    " - freada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkvSlyu3PfdW"
   },
   "source": [
    "# Visualize first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1UXoe6hPiFD"
   },
   "outputs": [],
   "source": [
    "def figure1(folder='train_dataset_iracing'):\n",
    "    cornering = Image.open(f'{folder}/curva_apex/curva (1).jpg')\n",
    "    braking = Image.open(f'{folder}/freada/freada (2).jpg')\n",
    "    accelerating = Image.open(f'{folder}/saida_curva/saida_curva (1).jpg')\n",
    "    straight = Image.open(f'{folder}/reta/reta (1).jpg')\n",
    "\n",
    "    images = [cornering, braking, accelerating, straight]\n",
    "    titles = ['cornering', 'braking', 'accelerating', 'straight']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 5))\n",
    "    for ax, image, title in zip(axs, images, titles):\n",
    "        ax.imshow(image)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(title)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "id": "y5ATw-ZBQolE",
    "outputId": "2c41c0ce-34bf-411d-dbc6-fb44d89e2791"
   },
   "outputs": [],
   "source": [
    "fig = figure1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meKydWgnRa8c"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11-Xx2mnRj8N"
   },
   "source": [
    "## ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2QEglzSUskY"
   },
   "outputs": [],
   "source": [
    "# Compose a sequence of preprocessing transforms\n",
    "# 1) Resize images to 28×28 pixels\n",
    "# 2) Ensure output is a PIL/torchvision Image (dropping any alpha channel)\n",
    "# 3) Convert pixel values to float32 and scale from [0–255] to [0.0–1.0]\n",
    "temp_transform = Compose([\n",
    "    Resize([28,28]),                        # Resize each image to 28×28\n",
    "    ToImage(),                         # Convert tensor back to PIL Image (enforces RGB)\n",
    "    ToDtype(torch.float32, scale=True) # Cast to float32 and normalize pixel range\n",
    "])\n",
    "\n",
    "# Create an ImageFolder dataset from the 'rps' directory\n",
    "# Images are grouped by subfolder name as class labels, and each image is transformed\n",
    "temp_dataset = ImageFolder(\n",
    "    root='train_dataset_iracing',\n",
    "    transform=temp_transform          # Apply the preprocessing pipeline to every image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZ6u3itMR8Ml",
    "outputId": "545c96a8-0b6c-49f4-8ac9-c168e0ca5690"
   },
   "outputs": [],
   "source": [
    "# the second element of this tuple is the label\n",
    "temp_dataset[0][0].shape, temp_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-9jjm5JSGed",
    "outputId": "c8521cfd-89e8-4a3e-e906-63f367f8a66b"
   },
   "outputs": [],
   "source": [
    "# Get total number of samples in the dataset\n",
    "dataset_size = len(temp_dataset)\n",
    "print(f\"Dataset size: {dataset_size} images\")\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(temp_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLQdRHeC_lv7"
   },
   "source": [
    "## Standardization\n",
    "\n",
    "To standardize data points, we need to learn their **mean** and **standard** deviation first.\n",
    "\n",
    "To compute these, we need to **load** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pjj2e3DLfSZ5"
   },
   "outputs": [],
   "source": [
    "temp_loader = DataLoader(temp_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4VuE0djcfRNk",
    "outputId": "b659c6d2-d349-48a7-9d5e-176cd78a2179"
   },
   "outputs": [],
   "source": [
    "# Each column represents a channel\n",
    "# first row is the number of data points\n",
    "# second row is the the sum of mean values\n",
    "# third row is the sum of standard deviations\n",
    "first_images, first_labels = next(iter(temp_loader))\n",
    "Architecture.statistics_per_channel(first_images, first_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUT_zNdw_scW",
    "outputId": "0e8fdc0b-ca8b-4869-d7f0-865306962c37"
   },
   "outputs": [],
   "source": [
    "# we can compute the average mean value and the average standard deviation, per channel.\n",
    "# make_normalizer() is a method that takes a data loader and returns an instance of the Normalize() transform\n",
    "normalizer = Architecture.make_normalizer(temp_loader)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save normalization values as normalize2 (as tensors)\n",
    "normalizer = Normalize(\n",
    "    mean=torch.tensor([0.2628, 0.2793, 0.2886]),\n",
    "    std=torch.tensor([0.2412, 0.2640, 0.2837]),\n",
    "    inplace=False\n",
    ")\n",
    "normalizer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9A_jux12RpOA"
   },
   "outputs": [],
   "source": [
    "# Define a pipeline of image transformations:\n",
    "# 1) Resize each image to 28×28 pixels\n",
    "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
    "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
    "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
    "composer = Compose([\n",
    "    Resize([28,28]),                         # Resize to 28×28\n",
    "    ToImage(),                          # Convert to PIL Image in RGB\n",
    "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
    "    normalizer                          # Apply custom normalization transform\n",
    "])\n",
    "\n",
    "# Instantiate training and validation datasets from folders:\n",
    "# - 'train_dataset_iracing' contains subfolders per class for training\n",
    "# - 'test_dataset_iracing' likewise for validation\n",
    "train_data = ImageFolder(root='train_dataset_iracing', transform=composer)\n",
    "val_data   = ImageFolder(root='test_dataset_iracing', transform=composer)\n",
    "\n",
    "# Wrap datasets in DataLoaders for batching and shuffling:\n",
    "# - batch_size=16 yields mini-batches of 16 images\n",
    "# - shuffle=True randomizes training order each epoch\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_data,   batch_size=16)  # no shuffle for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nf34Libu0nSW"
   },
   "outputs": [],
   "source": [
    "def figure2(first_images, first_labels):\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(12, 4))\n",
    "    titles = ['cornering', 'braking', 'accelerating', 'straight']\n",
    "    for i in range(6):\n",
    "        image, label = ToPILImage()(first_images[i]), first_labels[i]\n",
    "        axs[i].imshow(image)\n",
    "        axs[i].set_xticks([])\n",
    "        axs[i].set_yticks([])\n",
    "        axs[i].set_title(titles[label], fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150
    },
    "id": "UNrfKQsr0pQN",
    "outputId": "3ba38f4d-5f0e-447d-adc8-7fc9bd757e5d"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(88)\n",
    "first_images, first_labels = next(iter(train_loader))\n",
    "\n",
    "fig = figure2(first_images, first_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Base Model\n",
    "\n",
    "* n_features=5\n",
    "* dropout probability p=0.3\n",
    "* learn rate = 3e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model_cnn2 = CNN2(n_feature=5, p=0.3)\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer_cnn2 = optim.Adam(model_cnn2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_cnn2 = Architecture(model_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer_cnn2)\n",
    "arch_cnn2.set_loaders(train_loader, val_loader)\n",
    "arch_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(model_cnn2.state_dict(), 'base_model_cnn2.pth')\n",
    "fig = arch_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 54 minutes to go over 8 epochs - model with 4 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cnn2.conv1.weight.shape)\n",
    "fig = arch_cnn2.visualize_filters('conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cnn2.conv2.weight.shape)\n",
    "fig = arch_cnn2.visualize_filters('conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1','conv2', 'relu1', 'maxp1','maxp2', 'flatten']\n",
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "arch_cnn2.attach_hooks(\n",
    "  layers_to_hook=featurizer_layers + classifier_layers\n",
    ")\n",
    "\n",
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = arch_cnn2.predict(images_batch)\n",
    "predicted = np.argmax(logits, 1)\n",
    "\n",
    "arch_cnn2.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    fig_maps1 = arch_cnn2.visualize_outputs(featurizer_layers)\n",
    "    fig_maps2 = arch_cnn2.visualize_outputs(\n",
    "                  classifier_layers, y=labels_batch, yhat=predicted\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Architecture.loader_apply(val_loader,\n",
    "                          arch_cnn2.correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda x: x[0].item() / x[1].item())(Architecture.loader_apply(val_loader,\n",
    "                                                                arch_cnn2.correct).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get all predictions and true labels from the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "arch_cnn2.model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix using seaborn heatmap\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Task - base model + n_features change\n",
    "\n",
    "In this task we will analyze model output varying two times:\n",
    "\n",
    "1. n_feature = 3\n",
    "1. n_feature = 10\n",
    "\n",
    "we will keep dropout probability p=0.3 and lear rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model2_cnn2 = CNN2(n_feature=3, p=0.3) #changed n_feature from 5 to 3\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer2_cnn2 = optim.Adam(model2_cnn2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch2_cnn2 = Architecture(model2_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer2_cnn2)\n",
    "arch2_cnn2.set_loaders(train_loader, val_loader)\n",
    "arch2_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2_cnn2.state_dict(), 'model2_cnn2.pth')\n",
    "\n",
    "fig = arch2_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model 2 took 52 minutes to train its model in 8 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Visualize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1','conv2', 'relu1', 'maxp1','maxp2', 'flatten']\n",
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "arch2_cnn2.attach_hooks(\n",
    "  layers_to_hook=featurizer_layers + classifier_layers\n",
    ")\n",
    "\n",
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = arch2_cnn2.predict(images_batch)\n",
    "predicted = np.argmax(logits, 1)\n",
    "\n",
    "arch2_cnn2.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    fig_maps1 = arch2_cnn2.visualize_outputs(featurizer_layers)\n",
    "    fig_maps2 = arch2_cnn2.visualize_outputs(\n",
    "                  classifier_layers, y=labels_batch, yhat=predicted\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts = Architecture.loader_apply(val_loader, arch2_cnn2.correct)\n",
    "correct_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[147, 264],\n",
    "        [ 20, 196],\n",
    "        [695, 699],\n",
    "        [ 32, 254]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all predictions and true labels from the validation set\n",
    "all_preds2 = []\n",
    "all_labels2 = []\n",
    "\n",
    "arch2_cnn2.model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch2_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds2.extend(preds)\n",
    "        all_labels2.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(all_labels2, all_preds2)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Model 2 Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - n_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model3_cnn2 = CNN2(n_feature=10, p=0.3) #changed n_feature from 5 to 10\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer3_cnn2 = optim.Adam(model3_cnn2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch3_cnn2 = Architecture(model3_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer3_cnn2)\n",
    "arch3_cnn2.set_loaders(train_loader, val_loader)\n",
    "arch3_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 took 52 minutes to train 8 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 - Visualize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer_layers = ['conv1','conv2', 'relu1', 'maxp1','maxp2', 'flatten']\n",
    "classifier_layers = ['fc1', 'relu2', 'fc2']\n",
    "\n",
    "arch3_cnn2.attach_hooks(\n",
    "  layers_to_hook=featurizer_layers + classifier_layers\n",
    ")\n",
    "\n",
    "images_batch, labels_batch = next(iter(val_loader))\n",
    "logits = arch3_cnn2.predict(images_batch)\n",
    "predicted = np.argmax(logits, 1)\n",
    "\n",
    "arch3_cnn2.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context('seaborn-v0_8-whitegrid'):\n",
    "    fig_maps1 = arch3_cnn2.visualize_outputs(featurizer_layers)\n",
    "    fig_maps2 = arch3_cnn2.visualize_outputs(\n",
    "                  classifier_layers, y=labels_batch, yhat=predicted\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3_cnn2.state_dict(), 'model3_cnn2.pth')\n",
    "\n",
    "fig = arch3_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts3 = Architecture.loader_apply(val_loader, arch3_cnn2.correct)\n",
    "correct_counts3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[159, 264],\n",
    "        [ 68, 196],\n",
    "        [653, 699],\n",
    "        [ 50, 254]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions and true labels from the validation set\n",
    "all_preds3 = []\n",
    "all_labels3 = []\n",
    "\n",
    "arch3_cnn2.model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch3_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds3.extend(preds)\n",
    "        all_labels3.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3 = confusion_matrix(all_labels3, all_preds3)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining the models with shuffle = false\n",
    "\n",
    "Let's investigate the models outputs when not shuffling the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline of image transformations:\n",
    "# 1) Resize each image to 28×28 pixels\n",
    "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
    "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
    "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
    "composer = Compose([\n",
    "    Resize([28,28]),                         # Resize to 28×28\n",
    "    ToImage(),                          # Convert to PIL Image in RGB\n",
    "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
    "    normalizer                          # Apply custom normalization transform\n",
    "])\n",
    "\n",
    "# Instantiate training and validation datasets from folders:\n",
    "# - 'train_dataset_iracing' contains subfolders per class for training\n",
    "# - 'test_dataset_iracing' likewise for validation\n",
    "train_data = ImageFolder(root='train_dataset_iracing', transform=composer)\n",
    "val_data   = ImageFolder(root='test_dataset_iracing', transform=composer)\n",
    "\n",
    "# Wrap datasets in DataLoaders for batching and shuffling:\n",
    "# - batch_size=16 yields mini-batches of 16 images\n",
    "# - shuffle=True randomizes training order each epoch\n",
    "train_loader_2 = DataLoader(train_data, batch_size=16, shuffle=False)#change shuffle to False for train_loader_2\n",
    "val_loader   = DataLoader(val_data,   batch_size=16)  # no shuffle for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1\n",
    "torch.manual_seed(13)\n",
    "# Model/Architecture\n",
    "model_cnn2 = CNN2(n_feature=5, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "optimizer_cnn2 = optim.Adam(model_cnn2.parameters(), lr=3e-4)\n",
    "\n",
    "#Model 2\n",
    "# Model/Architecture\n",
    "model2_cnn2 = CNN2(n_feature=3, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "optimizer2_cnn2 = optim.Adam(model2_cnn2.parameters(), lr=3e-4)\n",
    "\n",
    "#Model 3\n",
    "# Model/Architecture\n",
    "model3_cnn2 = CNN2(n_feature=10, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "optimizer3_cnn2 = optim.Adam(model3_cnn2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1 Train\n",
    "arch_cnn2 = Architecture(model_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer_cnn2)\n",
    "arch_cnn2.set_loaders(train_loader_2, val_loader)\n",
    "arch_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it took 50 minutes to train model 1 without shuffling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 Train\n",
    "arch2_cnn2 = Architecture(model2_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer2_cnn2)\n",
    "arch2_cnn2.set_loaders(train_loader_2, val_loader)\n",
    "arch2_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3 Train\n",
    "arch3_cnn2 = Architecture(model3_cnn2,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer3_cnn2)\n",
    "arch3_cnn2.set_loaders(train_loader_2, val_loader)\n",
    "arch3_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1 Save\n",
    "torch.save(model_cnn2.state_dict(), 'model_cnn2_noshuffle.pth')\n",
    "\n",
    "fig = arch_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2 Save\n",
    "torch.save(model2_cnn2.state_dict(), 'model2_cnn2_noshuffle.pth')\n",
    "\n",
    "fig = arch2_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3 Save\n",
    "torch.save(model3_cnn2.state_dict(), 'model3_cnn2_noshuffle.pth')\n",
    "\n",
    "fig = arch3_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts = Architecture.loader_apply(val_loader, arch_cnn2.correct)\n",
    "correct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts2 = Architecture.loader_apply(val_loader, arch2_cnn2.correct)\n",
    "correct_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts3 = Architecture.loader_apply(val_loader, arch3_cnn2.correct)\n",
    "correct_counts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all predictions and true labels from the validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "all_preds2 = []\n",
    "all_labels2 = []\n",
    "\n",
    "all_preds3 = []\n",
    "all_labels3 = []\n",
    "\n",
    "arch_cnn2.model.eval()\n",
    "arch2_cnn2.model.eval()\n",
    "arch3_cnn2.model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch2_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds2.extend(preds)\n",
    "        all_labels2.extend(labels.cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = arch3_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds3.extend(preds)\n",
    "        all_labels3.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Model 1')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cm2 = confusion_matrix(all_labels2, all_preds2)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Model 2')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cm3 = confusion_matrix(all_labels3, all_preds3)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Blues', xticklabels=val_data.classes, yticklabels=val_data.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Model 3')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third task - Base model + new featurized blocks\n",
    "\n",
    "In order to execute this task it will be necessary to modify CNN2 class in to CNN3 above.\n",
    "\n",
    "This code addes two more featurizing block at the end of convulotional layer 2.\n",
    "\n",
    "However, to accommodate four identical convolutional blocks (Conv2d() with kernel_size=3 followed by MaxPool2d with kernel_size=2), the minimum feasible input image size is 124x124 pixels.\n",
    "\n",
    "The flatten layer remains the same:  `n_features * 5 * 5`\n",
    "\n",
    "With an input of this size, the feature map can be processed through all four blocks without its dimensions shrinking to a size smaller than the convolutional kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline of image transformations:\n",
    "# 1) Resize each image to 46×46 pixels\n",
    "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
    "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
    "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
    "composer = Compose([\n",
    "    Resize([124,124]),                         # Resize to 46×46\n",
    "    ToImage(),                          # Convert to PIL Image in RGB\n",
    "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
    "    normalizer                          # Apply custom normalization transform\n",
    "])\n",
    "\n",
    "# Instantiate training and validation datasets from folders:\n",
    "# - 'train_dataset_iracing' contains subfolders per class for training\n",
    "# - 'test_dataset_iracing' likewise for validation\n",
    "train_data_2 = ImageFolder(root='train_dataset_iracing', transform=composer)\n",
    "val_data_2   = ImageFolder(root='test_dataset_iracing', transform=composer)\n",
    "\n",
    "# Wrap datasets in DataLoaders for batching and shuffling:\n",
    "# - batch_size=16 yields mini-batches of 16 images\n",
    "# - shuffle=True randomizes training order each epoch\n",
    "train_loader_2 = DataLoader(train_data_2, batch_size=16, shuffle=True)\n",
    "val_loader_2   = DataLoader(val_data_2,   batch_size=16)  # no shuffle for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the second element of this tuple is the label\n",
    "train_data_2[0][0].shape, train_data_2[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was added in CNN3 below the third and forth convolutional block. These blocks are equal to \n",
    " \n",
    "``` Python\n",
    "        # Convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3(nn.Module):\n",
    "    def __init__(self, n_feature, p=0.0):\n",
    "        super(CNN3, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.p = p\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        \n",
    "        \n",
    "        # Creates the linear layers\n",
    "        # Where do this 5 * 5 come from?! Check it below\n",
    "        self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, 4)  # Changed from 3 to 4 classes\n",
    "        # Creates dropout layers\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # Featurizer\n",
    "        # First convolutional block\n",
    "        # 3@28x28 -> n_feature@26x26 -> max_pool2d n_feature@13x13\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #n_feature@13x13\n",
    "        \n",
    "        # Second convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        # n_feature@26x26 -> n_feature@13x13\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Forth convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Input dimension (n_feature@5x5)\n",
    "        # Output dimension (n_feature * 5 * 5)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        # Classifier\n",
    "\n",
    "        # Hidden Layer\n",
    "        # Input dimension (n_feature * 5 * 5)\n",
    "        # Output dimension (50)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        # Input dimension (50)\n",
    "        # Output dimension (4)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model4_cnn3 = CNN3(n_feature=5, p=0.3) #n_feature remains 5. Change CNN3\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer_cnn3 = optim.Adam(model4_cnn3.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch4_cnn2 = Architecture(model4_cnn3,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer_cnn3)\n",
    "arch4_cnn2.set_loaders(train_loader_2, val_loader_2)\n",
    "arch4_cnn2.train(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 4 Save\n",
    "torch.save(model4_cnn3.state_dict(), 'model4_cnn3.pth')\n",
    "\n",
    "fig = arch4_cnn2.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts4 = Architecture.loader_apply(val_loader_2, arch4_cnn2.correct)\n",
    "correct_counts4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it took 61 minutes to train model 4 during 8 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for Model 4\n",
    "all_preds4 = []\n",
    "all_labels4 = []\n",
    "arch4_cnn2.model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_2:\n",
    "        logits = arch4_cnn2.predict(images)\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        all_preds4.extend(preds)\n",
    "        all_labels4.extend(labels.cpu().numpy())\n",
    "cm4 = confusion_matrix(all_labels4, all_preds4)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm4, annot=True, fmt='d', cmap='Blues', xticklabels=val_data_2.classes, yticklabels=val_data_2.classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix - Model 4')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models\n",
    "\n",
    "Comparison among all 4 trained models in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models' weights\n",
    "model_cnn2.load_state_dict(torch.load('base_model_cnn2.pth'))\n",
    "model2_cnn2.load_state_dict(torch.load('model2_cnn2.pth'))\n",
    "model3_cnn2.load_state_dict(torch.load('model3_cnn2.pth'))\n",
    "model4_cnn3.load_state_dict(torch.load('model4_cnn3.pth'))\n",
    "\n",
    "# Set models to evaluation mode\n",
    "model_cnn2.eval()\n",
    "model2_cnn2.eval()\n",
    "model3_cnn2.eval()\n",
    "model4_cnn3.eval()\n",
    "\n",
    "# Helper function to compute accuracy\n",
    "def compute_accuracy(all_labels, all_preds):\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    return (all_labels == all_preds).sum() / len(all_labels)\n",
    "\n",
    "# Model 1 (CNN2, n_feature=5, 28x28)\n",
    "all_preds1, all_labels1 = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = model_cnn2(images)\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        all_preds1.extend(preds.cpu().numpy())\n",
    "        all_labels1.extend(labels.cpu().numpy())\n",
    "acc1 = compute_accuracy(all_labels1, all_preds1)\n",
    "\n",
    "# Model 2 (CNN2, n_feature=3, 28x28)\n",
    "all_preds2, all_labels2 = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = model2_cnn2(images)\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        all_preds2.extend(preds.cpu().numpy())\n",
    "        all_labels2.extend(labels.cpu().numpy())\n",
    "acc2 = compute_accuracy(all_labels2, all_preds2)\n",
    "\n",
    "# Model 3 (CNN2, n_feature=10, 28x28)\n",
    "all_preds3, all_labels3 = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        logits = model3_cnn2(images)\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        all_preds3.extend(preds.cpu().numpy())\n",
    "        all_labels3.extend(labels.cpu().numpy())\n",
    "acc3 = compute_accuracy(all_labels3, all_preds3)\n",
    "\n",
    "# Model 4 (CNN3, n_feature=5, 124x124)\n",
    "all_preds4, all_labels4 = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_2:\n",
    "        logits = model4_cnn3(images)\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        all_preds4.extend(preds.cpu().numpy())\n",
    "        all_labels4.extend(labels.cpu().numpy())\n",
    "acc4 = compute_accuracy(all_labels4, all_preds4)\n",
    "\n",
    "# Print insights\n",
    "print(f\"Model 1 (CNN2, n_feature=5, 28x28):   Accuracy = {acc1:.3f}\")\n",
    "print(f\"Model 2 (CNN2, n_feature=3, 28x28):   Accuracy = {acc2:.3f}\")\n",
    "print(f\"Model 3 (CNN2, n_feature=10, 28x28):  Accuracy = {acc3:.3f}\")\n",
    "print(f\"Model 4 (CNN3, n_feature=5, 124x124): Accuracy = {acc4:.3f}\")\n",
    "\n",
    "# Insight\n",
    "print(\"\\nInsight:\")\n",
    "print(\"Increasing the number of features from 3 to 5 to 10 in CNN2 shows how model capacity affects accuracy.\")\n",
    "print(\"Model 4 (CNN3) with deeper architecture and larger input size achieves different performance,\")\n",
    "print(\"highlighting the impact of both network depth and input resolution on classification accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Resizing image to 128x128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN4 CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN4(nn.Module):\n",
    "    def __init__(self, n_feature, p=0.0):\n",
    "        super(CNN4, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.p = p\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        # Creates the linear layers\n",
    "        # Where do this 115 * 63 come from?! Check it below\n",
    "        self.fc1 = nn.Linear(n_feature * 30 * 30, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  # Changed from 3 to 4 classes\n",
    "        # Creates dropout layers\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # Featurizer\n",
    "        # First convolutional block\n",
    "        # 3@28x28 -> n_feature@26x26 -> n_feature@13x13\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #maxpool kernel size 3\n",
    "        # Second convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #maxpool kernel size 2\n",
    "        # Input dimension (n_feature@5x5)\n",
    "        # Output dimension (n_feature * 5 * 5)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        # Classifier\n",
    "        # Hidden Layer\n",
    "        # Input dimension (n_feature * 5 * 5)\n",
    "        # Output dimension (50)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        # Output Layer\n",
    "        # Input dimension (50)\n",
    "        # Output dimension (4)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import image folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp transform for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compose a sequence of preprocessing transforms\n",
    "# 1) Resize images to 28×28 pixels\n",
    "# 2) Ensure output is a PIL/torchvision Image (dropping any alpha channel)\n",
    "# 3) Convert pixel values to float32 and scale from [0–255] to [0.0–1.0]\n",
    "temp_transform = Compose([\n",
    "    Resize([128,128]),                        # Resize each image to 28×28\n",
    "    ToImage(),                         # Convert tensor back to PIL Image (enforces RGB)\n",
    "    ToDtype(torch.float32, scale=True) # Cast to float32 and normalize pixel range\n",
    "])\n",
    "\n",
    "# Create an ImageFolder dataset from the 'rps' directory\n",
    "# Images are grouped by subfolder name as class labels, and each image is transformed\n",
    "temp_dataset = ImageFolder(\n",
    "    root='train_dataset_iracing',\n",
    "    transform=temp_transform          # Apply the preprocessing pipeline to every image\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 128, 128]), 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the second element of this tuple is the label\n",
    "temp_dataset[0][0].shape, temp_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7926 images\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Get total number of samples in the dataset\n",
    "dataset_size = len(temp_dataset)\n",
    "print(f\"Dataset size: {dataset_size} images\")\n",
    "\n",
    "# Get number of classes\n",
    "num_classes = len(temp_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_loader = DataLoader(temp_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute the average mean value and the average standard deviation, per channel.\n",
    "# make_normalizer() is a method that takes a data loader and returns an instance of the Normalize() transform\n",
    "normalizer_4 = Architecture.make_normalizer(temp_loader)\n",
    "normalizer_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 128x128 size images we got Mean and Std below:\n",
    "\n",
    "* mean=[tensor(0.2624), tensor(0.2789), tensor(0.2882)],\n",
    "* Std=[tensor(0.2756), tensor(0.2969), tensor(0.3162)],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=[tensor(0.2624), tensor(0.2789), tensor(0.2882)], std=[tensor(0.2655), tensor(0.2874), tensor(0.3076)], inplace=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save normalization values as normalize2 (as tensors)\n",
    "normalizer_4 = Normalize(\n",
    "    mean=torch.tensor([0.2624, 0.2789, 0.2882]),\n",
    "    std=torch.tensor([0.2655, 0.2874, 0.3076]),\n",
    "    inplace=False\n",
    ")\n",
    "normalizer_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline of image transformations:\n",
    "# 1) Resize each image to 480×270 pixels\n",
    "# 2) Ensure the output is a PIL/torchvision image (dropping any alpha channel)\n",
    "# 3) Cast pixels to float32 and scale from [0–255] to [0.0–1.0]\n",
    "# 4) Apply the user-defined normalization (e.g., mean/std normalization)\n",
    "composer = Compose([\n",
    "    Resize([128,128]),                         # Resize to 128×128\n",
    "    ToImage(),                          # Convert to PIL Image in RGB\n",
    "    ToDtype(torch.float32, scale=True), # Cast to float32 and normalize to [0,1]\n",
    "    normalizer_4                         # Apply custom normalization transform\n",
    "])\n",
    "\n",
    "# Instantiate training and validation datasets from folders:\n",
    "# - 'train_dataset_iracing' contains subfolders per class for training\n",
    "# - 'test_dataset_iracing' likewise for validation\n",
    "train_data = ImageFolder(root='train_dataset_iracing', transform=composer)\n",
    "val_data   = ImageFolder(root='test_dataset_iracing', transform=composer)\n",
    "\n",
    "# Wrap datasets in DataLoaders for batching and shuffling:\n",
    "# - batch_size=16 yields mini-batches of 16 images\n",
    "# - shuffle=True randomizes training order each epoch\n",
    "train_loader_3 = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader_3   = DataLoader(val_data,   batch_size=16)  # no shuffle for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "base_model2_cnn4 = CNN4(n_feature=5, p=0.3)\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer_cnn4 = optim.Adam(base_model2_cnn4.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_cnn4 = Architecture(base_model2_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        base_optimizer_cnn4)\n",
    "arch_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch_cnn4.train(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model2_cnn4.state_dict(), 'base_model2_cnn4.pth')\n",
    "\n",
    "fig = arch_cnn4.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 and 3 - changing number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model2_cnn4 = CNN4(n_feature=3, p=0.3) #changed n_feature from 5 to 3\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer2_cnn4 = optim.Adam(model2_cnn4.parameters(), lr=3e-4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch2_cnn4 = Architecture(model2_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer2_cnn4)\n",
    "arch2_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch2_cnn4.train(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2_cnn4.state_dict(), 'model2_cnn4.pth')\n",
    "\n",
    "fig = arch2_cnn4.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3\n",
    "torch.manual_seed(13)\n",
    "# Model/Architecture\n",
    "model3_cnn4 = CNN4(n_feature=10, p=0.3) #changed n_feature from 5 to 10\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "optimizer3_cnn4 = optim.Adam(model3_cnn4.parameters(), lr=3e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch3_cnn4 = Architecture(model3_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer3_cnn4)\n",
    "arch3_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch3_cnn4.train(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot losses for model 3\n",
    "torch.save(model3_cnn4.state_dict(), 'model3_cnn4.pth')\n",
    "fig = arch3_cnn4.plot_losses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "base_model2_cnn4 = CNN4(n_feature=5, p=0.3)\n",
    "model2_cnn4 = CNN4(n_feature=3, p=0.3) #changed n_feature from 5 to 3\n",
    "model3_cnn4 = CNN4(n_feature=10, p=0.3) #changed n_feature from 5 to 10\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer_cnn4 = optim.Adam(base_model2_cnn4.parameters(), lr=3e-4)\n",
    "optimizer2_cnn4 = optim.Adam(model2_cnn4.parameters(), lr=3e-4)   \n",
    "optimizer3_cnn4 = optim.Adam(model3_cnn4.parameters(), lr=3e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights for all three CNN4 models\n",
    "base_model2_cnn4.load_state_dict(torch.load('base_model2_cnn4.pth'))\n",
    "model2_cnn4.load_state_dict(torch.load('model2_cnn4.pth'))\n",
    "model3_cnn4.load_state_dict(torch.load('model3_cnn4.pth'))\n",
    "\n",
    "#Load Architecture\n",
    "arch_cnn4 = Architecture(base_model2_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        base_optimizer_cnn4)\n",
    "arch_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "\n",
    "arch2_cnn4 = Architecture(model2_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer2_cnn4)\n",
    "arch2_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "\n",
    "arch3_cnn4 = Architecture(model3_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        optimizer3_cnn4)\n",
    "arch3_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "base_model2_cnn4.eval()\n",
    "model2_cnn4.eval()\n",
    "model3_cnn4.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Increase number of blocks\n",
    "\n",
    "In order to execute this task it will be necessary to modify CNN4 class in to CNN5 below.\n",
    "\n",
    "This code addes two more featurizing block at the end of convulotional layer 2.\n",
    "\n",
    "However, to accommodate four identical convolutional blocks (Conv2d() with kernel_size=5 followed by MaxPool2d with kernel_size=2), the minimum feasible input image size is 64x64 pixels.\n",
    "\n",
    "Therefore, our size reduction to `128x128` still works.\n",
    "With an input of this size, the feature map can be processed through all four blocks without its dimensions shrinking to a size smaller than the convolutional kernel.\n",
    "\n",
    "The flatten layer remains the same:  `n_features * 5 * 5`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN5 Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was added in CNN3 below the third and forth convolutional block. These blocks are equal to \n",
    " \n",
    "``` Python\n",
    "        # Convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN5(nn.Module):\n",
    "    def __init__(self, n_feature, p=0.0):\n",
    "        super(CNN5, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.p = p\n",
    "        # Creates the convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(in_channels=n_feature,\n",
    "                               out_channels=n_feature,\n",
    "                               kernel_size=5)\n",
    "        \n",
    "        # Creates the linear layers\n",
    "        # input n_feature * 5 * 5 = 125\n",
    "        # output 128 neurons\n",
    "        self.fc1 = nn.Linear(n_feature * 5 * 5, 50)\n",
    "        self.fc2 = nn.Linear(50, 4)  # Changed from 3 to 4 classes\n",
    "        # Creates dropout layers\n",
    "        self.drop = nn.Dropout(self.p)\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        # Featurizer\n",
    "        # First convolutional block\n",
    "        # 3@28x28 -> n_feature@26x26 -> max_pool2d n_feature@13x13\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2) #n_feature@13x13\n",
    "        \n",
    "        # Second convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        # n_feature@26x26 -> n_feature@13x13\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Forth convolutional block\n",
    "        # n_feature * @13x13 -> n_feature@11x11 -> n_feature@5x5\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Input dimension (n_feature@5x5)\n",
    "        # Output dimension (n_feature * 5 * 5)\n",
    "        x = nn.Flatten()(x)\n",
    "        return x\n",
    "\n",
    "    def classifier(self, x):\n",
    "        # Classifier\n",
    "\n",
    "        # Hidden Layer\n",
    "        # Input dimension (n_feature * 5 * 5)\n",
    "        # Output dimension (50)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        # Input dimension (50)\n",
    "        # Output dimension (4)\n",
    "        if self.p > 0:\n",
    "            x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.featurizer(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4\n",
    "torch.manual_seed(13)\n",
    "\n",
    "# Model/Architecture\n",
    "model4_cnn5 = CNN5(n_feature=5, p=0.3)\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer4_cnn5 = optim.Adam(model4_cnn5.parameters(), lr=3e-4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "arch4_cnn5 = Architecture(model4_cnn5,\n",
    "                          multi_loss_fn,\n",
    "                          optimizer4_cnn5)\n",
    "arch4_cnn5.set_loaders(train_loader_3, val_loader_3)\n",
    "arch4_cnn5.train(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot losses for model 4\n",
    "torch.save(model4_cnn5.state_dict(), 'model4_cnn5.pth')\n",
    "fig = arch4_cnn5.plot_losses()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure model 4\n",
    "model4_cnn5 = CNN5(n_feature=5, p=0.3) #n_feature remains 5. Change CNN5\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "optimizer4_cnn5 = optim.Adam(model4_cnn5.parameters(), lr=3e-4)\n",
    "\n",
    "#Load model 4\n",
    "model4_cnn5.load_state_dict(torch.load('model4_cnn5.pth'))\n",
    "\n",
    "#load architecture for model 4\n",
    "arch4_cnn5 = Architecture(model4_cnn5,\n",
    "                          multi_loss_fn,\n",
    "                          optimizer4_cnn5)\n",
    "arch4_cnn5.set_loaders(train_loader_3, val_loader_3)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model4_cnn5.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts = Architecture.loader_apply(val_loader_3, arch_cnn4.correct)\n",
    "correct_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts2 = Architecture.loader_apply(val_loader_3, arch2_cnn4.correct)\n",
    "correct_counts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts3 = Architecture.loader_apply(val_loader_3, arch3_cnn4.correct)\n",
    "correct_counts3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_counts4 = Architecture.loader_apply(val_loader_3, arch4_cnn5.correct)\n",
    "correct_counts4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of data to iterate through\n",
    "models_data = [correct_counts, correct_counts2, correct_counts3, correct_counts4]\n",
    "accuracies = []\n",
    "\n",
    "# Calculate the accuracy for each model\n",
    "for data in models_data:\n",
    "    total_correct = data[:, 0].sum().item()\n",
    "    total_samples = data[:, 1].sum().item()\n",
    "    accuracy = (total_correct / total_samples) * 100  # Convert to percentage\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# Step 2: Put image text in English\n",
    "model_names = ['Model 1', 'Model 2', 'Model 3', 'Model 4']\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "bars = ax.bar(model_names, accuracies, color=['#1f77b4', '#ff7f0e', '#2ca02c','#d62728'],)\n",
    "\n",
    "# Add title and labels in English\n",
    "ax.set_title('Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_ylim(0, max(accuracies) + 10) # Set Y-axis limit slightly above the highest bar\n",
    "ax.yaxis.grid(False)\n",
    "ax.set_yticklabels([]) #remove y-tick labels for cleaner look\n",
    "\n",
    "# Add the exact value on top of each bar\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2.0, yval + 1.0, f'{yval:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Improve layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure to a file\n",
    "plt.savefig('model_accuracy_comparison.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the calculated values in English\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"{model_names[i]}: Accuracy of {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from graph above that the variation in number of feature (n) changes the model accuracy. The best model had accuracy of 65%, which is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "In this section we will analyze the confusion matrix of the best model - `base_model_cnn2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "# Model/Architecture\n",
    "base_model2_cnn4 = CNN4(n_feature=5, p=0.3)\n",
    "\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "base_optimizer_cnn4 = optim.Adam(base_model2_cnn4.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load base model\n",
    "base_model2_cnn4.load_state_dict(torch.load('base_model2_cnn4.pth'))\n",
    "#load architecture\n",
    "arch_cnn4 = Architecture(base_model2_cnn4,\n",
    "                        multi_loss_fn,\n",
    "                        base_optimizer_cnn4)\n",
    "arch_cnn4.set_loaders(train_loader_3, val_loader_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- 1. Setup ---\n",
    "# Define the device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move your model to the correct device\n",
    "base_model2_cnn4.to(device)\n",
    "\n",
    "# --- 2. Collect Predictions and True Labels ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "base_model2_cnn4.eval()\n",
    "\n",
    "# Disable gradient calculations for inference\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader_3:\n",
    "        # Move data to the same device as the model\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        logits = base_model2_cnn4(images)\n",
    "        \n",
    "        # Get the predicted class index\n",
    "        preds = torch.argmax(logits, axis=1)\n",
    "        \n",
    "        # Move data back to the CPU for compatibility with numpy and sklearn\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# --- 3. Compute and Plot the Confusion Matrix ---\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Define your English labels\n",
    "# IMPORTANT: This list order MUST match the order of your dataset's classes\n",
    "# For example, if val_loader.dataset.classes is ['curva_apex', 'freada', 'reta', 'saida_curva']\n",
    "# then the order below should correspond to that.\n",
    "english_labels = ['Mid Corner', 'Braking', 'Straight', 'Exit Corner']\n",
    "\n",
    "# Plot the confusion matrix using seaborn\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=english_labels,\n",
    "            yticklabels=english_labels,\n",
    "            annot_kws={\"size\": 14}) # Adjust font size of the numbers\n",
    "\n",
    "# Add titles and labels for clarity\n",
    "plt.title('Confusion Matrix - Base Model', fontsize=16)\n",
    "plt.xlabel('Predicted Label', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=14)\n",
    "\n",
    "# Rotate labels for better readability\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout() # Adjust layout to make sure everything fits\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Learning Rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(11)\n",
    "new_base_model = CNN4(n_feature=5, p=0.3)\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "new_optimizer = optim.Adam(new_base_model.parameters(), lr=3e-4)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "lr_finder = LRFinder(\n",
    "    new_base_model, new_optimizer, multi_loss_fn, device=device\n",
    ")\n",
    "lr_finder.range_test(train_loader_3, end_lr=1e-1, num_iter=100)\n",
    "lr_finder.plot(log_lr=True)\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LR Finder with our models, it suggests to use lambda rate = `3.53e-3`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - Retraining with new lambda rate\n",
    "\n",
    "In this task we will calculate the training losses using the suggested lambda rate of 3.53E-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the suggested learning rate for all optimizers\n",
    "suggested_lr = 3.53e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all four models and their architectures for evaluation\n",
    "\n",
    "## Model 1\n",
    "base_model2_cnn4 = CNN4(n_feature=5, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "new_base_optimizer_cnn4 = optim.Adam(base_model2_cnn4.parameters(), lr=suggested_lr)    \n",
    "# Load model 1\n",
    "base_model2_cnn4.load_state_dict(torch.load('base_model2_cnn4.pth'))\n",
    "#Architecture\n",
    "arch_cnn4 = Architecture(base_model2_cnn4, multi_loss_fn, base_optimizer_cnn4)\n",
    "arch_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "base_model2_cnn4.eval()\n",
    "\n",
    "\n",
    "# Model 2\n",
    "model2_cnn4 = CNN4(n_feature=3, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "new_optimizer2_cnn4 = optim.Adam(model2_cnn4.parameters(), lr=suggested_lr)\n",
    "# Load model 2   \n",
    "model2_cnn4.load_state_dict(torch.load('model2_cnn4.pth'))\n",
    "arch2_cnn4 = Architecture(model2_cnn4, multi_loss_fn, optimizer2_cnn4)\n",
    "arch2_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "model2_cnn4.eval()\n",
    "\n",
    "# Model 3\n",
    "model3_cnn4 = CNN4(n_feature=10, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "new_optimizer3_cnn4 = optim.Adam(model3_cnn4.parameters(), lr=suggested_lr)\n",
    "# Load model 3\n",
    "model3_cnn4.load_state_dict(torch.load('model3_cnn4.pth'))\n",
    "arch3_cnn4 = Architecture(model3_cnn4, multi_loss_fn, optimizer3_cnn4)\n",
    "arch3_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "model3_cnn4.eval()\n",
    "\n",
    "# Model 4\n",
    "model4_cnn5 = CNN5(n_feature=5, p=0.3)\n",
    "# Loss function\n",
    "multi_loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Optimizer\n",
    "new_optimizer4_cnn5 = optim.Adam(model4_cnn5.parameters(), lr=suggested_lr)\n",
    "# Load model 4\n",
    "model4_cnn5.load_state_dict(torch.load('model4_cnn5.pth'))\n",
    "#Architecture\n",
    "arch4_cnn5 = Architecture(model4_cnn5, multi_loss_fn, optimizer4_cnn5)\n",
    "arch4_cnn5.set_loaders(train_loader_3, val_loader_3)\n",
    "model4_cnn5.eval()\n",
    "\n",
    "#print models layers\n",
    "print(\"Model 1 (CNN4, n_feature=5):\")\n",
    "for name, param in base_model2_cnn4.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "print(\"\\nModel 2 (CNN4, n_feature=3):\")\n",
    "for name, param in model2_cnn4.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "print(\"\\nModel 3 (CNN4, n_feature=10):\")\n",
    "for name, param in model3_cnn4.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\") \n",
    "print(\"\\nModel 4 (CNN5, n_feature=5):\")\n",
    "for name, param in model4_cnn5.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Model 1 (n_feature=5)\n",
    "arch_cnn4 = Architecture(base_model2_cnn4, multi_loss_fn, new_base_optimizer_cnn4)\n",
    "arch_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch_cnn4.train(7)\n",
    "\n",
    "# Save the retrained base model\n",
    "torch.save(base_model2_cnn4.state_dict(), 'retrained_base_model2_cnn4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Model 2 (n_feature=3)\n",
    "arch2_cnn4 = Architecture(model2_cnn4, multi_loss_fn, new_optimizer2_cnn4)\n",
    "arch2_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch2_cnn4.train(7)\n",
    "\n",
    "#save the retrained model 2\n",
    "torch.save(model2_cnn4.state_dict(), 'retrained_model2_cnn4.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Model 3 (n_feature=10)\n",
    "arch3_cnn4 = Architecture(model3_cnn4, multi_loss_fn, new_optimizer3_cnn4)\n",
    "arch3_cnn4.set_loaders(train_loader_3, val_loader_3)\n",
    "arch3_cnn4.train(7)\n",
    "\n",
    "#save the retrained model 3\n",
    "torch.save(model3_cnn4.state_dict(), 'retrained_model3_cnn4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Model 4 (n_feature=5)\n",
    "arch4_cnn5 = Architecture(model4_cnn5, multi_loss_fn, new_optimizer4_cnn5)\n",
    "arch4_cnn5.set_loaders(train_loader_3, val_loader_3)\n",
    "arch4_cnn5.train(7)\n",
    "\n",
    "#save the retrained model 4\n",
    "torch.save(model4_cnn5.state_dict(), 'retrained_model4_cnn5.pth')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot losses for retrained models\n",
    "fig = arch_cnn4.plot_losses()\n",
    "#fig = arch2_cnn4.plot_losses()\n",
    "#fig = arch3_cnn4.plot_losses()\n",
    "#fig = arch4_cnn5.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the retrained models\n",
    "correct_counts_retrained = Architecture.loader_apply(val_loader_3, arch_cnn4.correct)\n",
    "correct_counts_retrained2 = Architecture.loader_apply(val_loader_3, arch2_cnn4.correct)\n",
    "correct_counts_retrained3 = Architecture.loader_apply(val_loader_3, arch3_cnn4.correct)\n",
    "correct_counts_retrained4 = Architecture.loader_apply(val_loader_3, arch4_cnn5.correct)\n",
    "# Calculate accuracy for retrained models\n",
    "def calculate_accuracy(correct_counts):\n",
    "    total_correct = correct_counts[:, 0].sum().item()\n",
    "    total_samples = correct_counts[:, 1].sum().item()\n",
    "    return (total_correct / total_samples) * 100  # Convert to percentage\n",
    "acc_retrained = calculate_accuracy(correct_counts_retrained)\n",
    "acc_retrained2 = calculate_accuracy(correct_counts_retrained2)\n",
    "acc_retrained3 = calculate_accuracy(correct_counts_retrained3)\n",
    "acc_retrained4 = calculate_accuracy(correct_counts_retrained4)\n",
    "# Print retrained model accuracies\n",
    "print(f\"Re-trained Model 1 (n_feature=5):   Accuracy = {acc_retrained:.2f}%\")\n",
    "print(f\"Re-trained Model 2 (n_feature=3):   Accuracy = {acc_retrained2:.2f}%\")\n",
    "print(f\"Re-trained Model 3 (n_feature=10):  Accuracy = {acc_retrained3:.2f}%\")\n",
    "print(f\"Re-trained Model 4 (n_feature=5):   Accuracy = {acc_retrained4:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOtVk2h00bhyPmj+u7m8ey9",
   "gpuType": "T4",
   "include_colab_link": true,
   "mount_file_id": "1_qWyCblfF8B7RXdqjIl202iKnbosspKp",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
