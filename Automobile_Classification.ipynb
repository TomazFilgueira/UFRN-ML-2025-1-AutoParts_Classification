{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Automobile Classification.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomazFilgueira/UFRN-ML-2025-1-AutoParts_Classification/blob/main/Automobile_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "mdwaquarazam_automobilepartsindentification_path = kagglehub.dataset_download('mdwaquarazam/automobilepartsindentification')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "dbtNUEL4FgVo",
        "outputId": "fa549124-6d44-42c8-fb9b-fc19ea40dd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/mdwaquarazam/automobilepartsindentification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 38.1M/38.1M [00:02<00:00, 14.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification of Automobile parts\n",
        "\n",
        "This notebook builds an end-end multiclass image classifier using Tensorflow 2.0 and TensorFlow Hub.\n",
        "\n",
        "## 1. Problem\n",
        "  Identifying the automobile parts given an image.\n",
        "\n",
        "## 2.The data we are using is from `Kaggle`\n",
        "https://www.kaggle.com/datasets/mdwaquarazam/automobilepartsindentification\n",
        "\n",
        "## 3. Evaluation\n",
        "For each image in the test set, you must predict a probability for each of the different breeds. The evaluation is the file with the prediction probabilities with the image of each dog of each test image.\n",
        "\n",
        "## 4.Features\n",
        "there are 689 images in the dataset and we will use 50% of the image to train the model.these data is having a label. Test images are not having any labels."
      ],
      "metadata": {
        "id": "SObfdxgI4Mah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get our workspace ready"
      ],
      "metadata": {
        "id": "7aQzIhhJATAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Tensorflow into Colab\n",
        "# Import Tensorflow Hub\n",
        "# Make sure we are using GPU.\n",
        "# Importing necessary tools"
      ],
      "metadata": {
        "id": "67hF9V4f4C16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "print(\"TensorFlow Version is : \" ,tf.__version__)\n",
        "print(\"TensorFlow Hub Version: \", hub.__version__)"
      ],
      "metadata": {
        "id": "ErwQIAb4Av_g",
        "outputId": "0a9c30fc-6a4d-45ac-a8d5-6dd6fb950917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version is :  2.18.0\n",
            "TensorFlow Hub Version:  0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU Availability\n",
        "print(\"GPU is available (YESSSSSSSS !!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"Not available\")"
      ],
      "metadata": {
        "id": "4-Qk7qERAyIr",
        "outputId": "f8738d80-b95d-4ee0-ea4d-a1d0bcfdc858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available (YESSSSSSSS !!!!!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting our data ready , turing them into **Tensors**."
      ],
      "metadata": {
        "id": "QeqfTmMwCNen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checkout the labels of our data.\n",
        "import pandas as pd\n",
        "data_dir ='../input/automobilepartsindentification/Automobile-parts,.Typecast.csv'\n",
        "labels = pd.read_csv(data_dir)\n",
        "print(labels.describe)"
      ],
      "metadata": {
        "id": "oTyiCdbZA2qP",
        "outputId": "c042f970-79cf-4564-9d71-7b95ddfed58e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/automobilepartsindentification/Automobile-parts,.Typecast.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3309321477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'../input/automobilepartsindentification/Automobile-parts,.Typecast.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/automobilepartsindentification/Automobile-parts,.Typecast.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many images we have per Automobile category.\n",
        "labels.Type.value_counts().plot.bar(figsize = (30,20), color ='SeaGreen')\n"
      ],
      "metadata": {
        "id": "pbhNP5_qCo7s",
        "outputId": "6508f729-e9e2-4b46-de6c-6a7b28362dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-2844936740.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#How many images we have per Automobile category.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'SeaGreen'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels.Type.value_counts().median()"
      ],
      "metadata": {
        "id": "4NOkEk0kC-2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets view images of each category in next few line items\n",
        "from IPython.display import Image\n"
      ],
      "metadata": {
        "id": "5Pm3rLzYIBwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting images and all their labels\n",
        "Lets get the images and all their labels"
      ],
      "metadata": {
        "id": "Lt-G1oynOjEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels.head()"
      ],
      "metadata": {
        "id": "cXX_TIKaPaFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pathnames from Image ID's\n",
        "filename = ['../input/automobilepartsindentification/Automobile-parts/Train/'+fname for fname in labels[\"ID\"]]"
      ],
      "metadata": {
        "id": "B0r1LYwbPb7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Check whther number of filename matches with actual image files\n",
        "import os\n",
        "if len(os.listdir(\"'../input/automobilepartsindentification/Automobile-parts'/Train\")) == len(filename):\n",
        "  print(\"File names match actual amount of files, proceed!!!\")\n",
        "else:\n",
        "   print(\"File names does not match actual amount of files, check the target directory!!\")"
      ],
      "metadata": {
        "id": "YQOzr440RYfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image(filename[120])"
      ],
      "metadata": {
        "id": "KwYEe2dmtNl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We have got our training images filepaths in a list, let's prepare our labels"
      ],
      "metadata": {
        "id": "OiVivmP7s9HO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "labelnames = labels[\"Type\"].to_numpy()\n",
        "#labelnames = np.array(labelnames) ## does same thing as above"
      ],
      "metadata": {
        "id": "t2iwuWPFs9tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## See if any number of labels matches the number of file names\n",
        "if len(labelnames) == len(filename):\n",
        "  print(\"Number of labels and number of filenames are matching, Proceed!!!\")\n",
        "else:\n",
        "  print(\"Number of labels and number of filenames are not matching, please check the labels\")"
      ],
      "metadata": {
        "id": "UCZAKxS7tXkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find Unique label values\n",
        "unique_labels = np.unique(labelnames)\n",
        "unique_labels"
      ],
      "metadata": {
        "id": "CW3-ElcHtZbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelnames[0], unique_labels[0]"
      ],
      "metadata": {
        "id": "i1N9z6VsthlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn a single label into an array of booleans\n",
        "print(labelnames[0])\n",
        "labelnames[0] == unique_labels\n"
      ],
      "metadata": {
        "id": "3-NakKB4trHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn every label into Boolean array\n",
        "boolean_labels = [label == unique_labels for label in labelnames]\n",
        "len(boolean_labels)\n",
        "print(boolean_labels)"
      ],
      "metadata": {
        "id": "rZ99S_RyuV0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turing Boolean Array into integer\n",
        "print(labelnames[300]) # Original label\n",
        "print(np.where(unique_labels == labelnames[1])) # Index where label occurs\n",
        "print(boolean_labels[300].argmax()) # Index where label occurs in boolean array\n",
        "print(boolean_labels[300].astype(int)) # There will be a 1 where the sample label occurs"
      ],
      "metadata": {
        "id": "RWMXngw2u2nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our own validation set\n",
        "### we are creating our own validation set as data from Kaggle does not come with validation"
      ],
      "metadata": {
        "id": "7bi7CT_jwEPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup X and y\n",
        "X = filename\n",
        "y = boolean_labels"
      ],
      "metadata": {
        "id": "8HPHDuYjwFJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "id": "clhEDKoGwfIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We are starting with 40% of the training data set and will increase as needed.\n"
      ],
      "metadata": {
        "id": "3QZazS_Nwts-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_IMAGES = 270 #@param {type:\"slider\", min:150, max:350, step:30}"
      ],
      "metadata": {
        "id": "h48odT6Nw134"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's split the data with training and validation dataset.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Split them into training and validation of total size of NUM_IMAGE\n",
        "X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES], y[:NUM_IMAGES],\n",
        "                                                  test_size =0.1,\n",
        "                                                  random_state =42)\n",
        "len(X_train), len(y_train) , len(X_val), len(y_val)"
      ],
      "metadata": {
        "id": "ynfTWEraxNbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:2], y_val[:2]"
      ],
      "metadata": {
        "id": "yLDX0SNrQgsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[:2], y_train[:2]"
      ],
      "metadata": {
        "id": "OO2gGAfFx-YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Images - Turning them into **Tensors**\n",
        "\n",
        "To preprocess image into Tensors, we are going to write a function which does following things:\n",
        "* Take an image file as input\n",
        "* Use Tensorflow to read the file save in the variable called `Image`\n",
        "* Turn our image into Tensor ( we will convert into PNG).\n",
        "* Resize the image to be the shape of 224,224\n",
        "* Return the modified Image.\n",
        "\n",
        "\n",
        "Lets see what importing an image looks like."
      ],
      "metadata": {
        "id": "iFOvTC1kzZPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert an image to a numby array\n",
        "from matplotlib.pyplot import imread\n",
        "image = imread(filename[300])\n",
        "image.shape"
      ],
      "metadata": {
        "id": "o7LZNrLszGrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image[::2]"
      ],
      "metadata": {
        "id": "CMgSQi3L2b2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn image into tensor\n",
        "tf.constant(image)"
      ],
      "metadata": {
        "id": "QrN87tJr2lkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define size\n",
        "IMG_SIZE = 224\n",
        "# Create a function for pre-processing image\n",
        "def process_image(image_path, image_size = IMG_SIZE):\n",
        "  \"\"\"\n",
        "  Takes an image path and turns that image to Tensor\n",
        "  \"\"\"\n",
        "  #Read in an image file\n",
        "  image = tf.io.read_file(image_path)\n",
        "\n",
        "  #Turn the image to PNG formated numerical Tensor with 3 color channels(Red, Green and Blue)\n",
        "  image = tf.image.decode_png(image, channels =3)\n",
        "\n",
        "  # Convert the color channel values from 0-255 to 0-1\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  #Resize the image to our desired value (224, 224)\n",
        "\n",
        "  image = tf.image.resize(image, size = [IMG_SIZE, IMG_SIZE])\n",
        "  return image\n"
      ],
      "metadata": {
        "id": "jn2bXAgu2wDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turning data into batches\n"
      ],
      "metadata": {
        "id": "RjLJyANx4p3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We need to turn all the data into data batches. we will make a batch of 30 images at a time to avoid any utilization issue and will convert them into batches.\n",
        "In order to use tensor flow effectively,we need our data to be in the form of tensor tuples which may look like `(image, label)`"
      ],
      "metadata": {
        "id": "pfAEj2Oc59e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple function to return tuple\n",
        "def get_image_label(image_path, label):\n",
        "  \"\"\"\n",
        "  Takes an image path and associated label, process them and returns a tupe of format ( image, label)\n",
        "  \"\"\"\n",
        "  image = process_image(image_path)\n",
        "  return image, label\n"
      ],
      "metadata": {
        "id": "CsnWxAxs4mL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# demo of the above:\n",
        "process_image(X[300]), tf.constant(y[300])"
      ],
      "metadata": {
        "id": "Pp8flMrx62DN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we have got a way to turn our data into Tensors in the form ( image, label). Let's make a function to turn all the data ( X , y) into batches"
      ],
      "metadata": {
        "id": "yLZjHnPC8MGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the batch size as 30\n",
        "BATCH_SIZE = 30\n",
        "\n",
        "#Create a function to turn data into batches\n",
        "def create_data_batches(X, y= None, batch_size = BATCH_SIZE, valid_data = False, test_data = False ):\n",
        "  \"\"\"\n",
        "  Create a data of batches out of image (X) and label (y) pairs.\n",
        "  Shuffles the data if it is training data, does not shuffle it if it is not valid data\n",
        "  Also, accepts test data as input.\n",
        "  \"\"\"\n",
        "  # if the data set is test data set, then probably w don't have labels.\n",
        "  if test_data:\n",
        "    print(\"Creating test data batches......\")\n",
        "    data = tf.data.Dataset.from_tensor_slices(tf.constant(X)) # Only file paths, no labels.\n",
        "    data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "  #if the dataset is a valid dataset, we don't need to shuffle it.\n",
        "  elif valid_data:\n",
        "    print(\"Creating valid data batches....\")\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                                      tf.constant(y)))\n",
        "    data_batch=data.map(get_image_label).batch(BATCH_SIZE)\n",
        "    return data_batch\n",
        "  else:\n",
        "    print(\"Creating Training Data batches ...........\")\n",
        "    # Turn file paths and labels into Tensors.\n",
        "    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),\n",
        "                                                      tf.constant(y)))\n",
        "\n",
        "    #Shuffling the pathnames and labels before mapping image proceesor functions is faster than shuffling image\n",
        "    data= data.shuffle(buffer_size = len(X))\n",
        "\n",
        "    # Create (image, label) tuple and it also turns the image path into a preprocessing image\n",
        "    data = data.map(get_image_label)\n",
        "\n",
        "    #turn training data into batches\n",
        "    data_batch = data.batch(BATCH_SIZE)\n",
        "    return data_batch"
      ],
      "metadata": {
        "id": "wBeRuys_7sY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create training and validation data batches\n",
        "train_data = create_data_batches(X_train, y_train)\n",
        "val_data = create_data_batches(X_val, y_val , valid_data = True)"
      ],
      "metadata": {
        "id": "lLdObRBCIvkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, val_data"
      ],
      "metadata": {
        "id": "PZIAxxSlJGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing different aspect of data batches\n",
        "train_data.element_spec"
      ],
      "metadata": {
        "id": "E23-LevoRIOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Our training data is in batches now, hence it is diffcult to understand,let's visualize it."
      ],
      "metadata": {
        "id": "CCuoVdLJSG_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#create a function to view 30 images as batch\n",
        "def show_30_images(images,labels):\n",
        "  \"\"\"\n",
        "    Shows 30 images along with their labels\n",
        "  \"\"\"\n",
        "  #Setup figure\n",
        "  plt.figure(figsize=(20,20))\n",
        "  #Loop through 30 to show 30 images\n",
        "  for i in range(25):\n",
        "    #Create subplots\n",
        "    ax = plt.subplot(5,5, i+1)\n",
        "    #Display image\n",
        "    plt.imshow(images[i])\n",
        "    plt.title(unique_labels[labels[i].argmax()])\n",
        "    #turn gridlines off\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "TWVlOZdrRS09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if any file is not supported image type\n",
        "from pathlib import Path\n",
        "import imghdr\n",
        "\n",
        "data_dir = \"drive/MyDrive/Automobile/Train/\"\n",
        "image_extensions = [\".png\", \".jpg\"]  # add there all your images file extensions\n",
        "\n",
        "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
        "for filepath in Path(data_dir).rglob(\"*\"):\n",
        "    if filepath.suffix.lower() in image_extensions:\n",
        "        img_type = imghdr.what(filepath)\n",
        "        if img_type is None:\n",
        "            print(f\"{filepath} is not an image\")\n",
        "        elif img_type not in img_type_accepted_by_tf:\n",
        "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
      ],
      "metadata": {
        "id": "wEncBoZnhh5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets visualize the data in training batch\n",
        "train_images, train_labels = next(train_data.as_numpy_iterator())\n",
        "show_30_images(train_images, train_labels)"
      ],
      "metadata": {
        "id": "rLjWCj8ji-Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RabGdYX0Surg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's visualize our validation set\n",
        "val_images, val_label = next(val_data.as_numpy_iterator())\n",
        "show_30_images(val_images,val_label)"
      ],
      "metadata": {
        "id": "eu8yHn4ITov8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oz8QQO26piRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build a model.\n",
        "We need to define below mentioned information before building a model.\n",
        "\n",
        "1) The Input shape ( shape of our images in form of Tensors).\n",
        "\n",
        "2) The Output shape( Image labels in the form of Tensor) of our model.\n",
        "\n",
        "3) The URL model which we want to use from Tensorflow Hub https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\n"
      ],
      "metadata": {
        "id": "liagl98Iw2GS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup Input shape for the model.\n",
        "INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] #[BATCH, HEIGHT, WIDTH, COLOR]\n",
        "OUTPUT_SHAPE = len(unique_labels)\n"
      ],
      "metadata": {
        "id": "JIyweW0oy8hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup model URL from TensorFlow HUB\n",
        "MODEL_URL =\"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\""
      ],
      "metadata": {
        "id": "MJZLa6VP36TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we have received the input, output and the model ready to go, let's put them together in a keras deep learning. Let's create a function.\n",
        "\n",
        "* Create a function providing input,output and model.\n",
        "* define the layers in Keras model in sequantial function.\n",
        "* Compile the model ( it should be evaluated and improved).\n",
        "* Returns the model. Steps can be found in https://www.tensorflow.org/guide/keras/sequential_model\n"
      ],
      "metadata": {
        "id": "L1qElB8d2qrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creates a function which builds a Keras model.\n",
        "def create_model(input_shape = INPUT_SHAPE, Output_shape = OUTPUT_SHAPE, model_url = MODEL_URL):\n",
        "  \"\"\"\n",
        "  Create a Tensorflow Keras Sequantial model\n",
        "  \"\"\"\n",
        "  print(f\"Building model with: {MODEL_URL}\")\n",
        "\n",
        "  #Setup model layers:\n",
        "  model = tf.keras.Sequential(\n",
        "      [\n",
        "            hub.KerasLayer(MODEL_URL), #1st Layer (Input Layer)\n",
        "            tf.keras.layers.Dense(units = OUTPUT_SHAPE,\n",
        "                                  activation = \"softmax\")]) #2nd Layer(Output Layer)\n",
        "  #Compile the model\n",
        "  model.compile(loss = tf.keras.losses.CategoricalCrossentropy(),\n",
        "                 optimizer = tf.keras.optimizers.Adam(),\n",
        "                 metrics =[\"accuracy\"])\n",
        "\n",
        "  #Build model.\n",
        "  model.build(INPUT_SHAPE)\n",
        "  return model\n",
        "\n"
      ],
      "metadata": {
        "id": "zbCc1Q3BzksZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i8zp3QTY384b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating call backs.\n",
        "call backs are helper functions a model can use during training to save its progress/ stop training if the model stops improving.\n",
        "we will create 2 call back functions ( 1 for Tensorboards) which will help to track the progress and other for early stopping for preventing our model from training for too long."
      ],
      "metadata": {
        "id": "68an51qI7lJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorboard callback\n",
        "To load Tensorboard call back, we need to perform 3 things:\n",
        "* Load TensorBoard notebook extension.\n",
        "* Create a Tensorboard callback which will save logs to a directory and pass it our models `fit()` function.\n",
        "* Visualize our model's training logs with `%tensorboard` magic function"
      ],
      "metadata": {
        "id": "HbQJvB4h80Dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import Tensorboard magic function\n",
        "%load_ext tensorboard\n"
      ],
      "metadata": {
        "id": "xEWnSkZy6_xZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "#Create a Tensorboard callback which will save logs to a directory and pass it our models fit() function.\n",
        "def create_tensorboard_callback():\n",
        "  #Create a log directory to save tensorboard logs\n",
        "  logdir = os.path.join(\"drive/MyDrive/Automobile/logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir = logdir)\n",
        ""
      ],
      "metadata": {
        "id": "9qpZefEI-5Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a function to build a early stopping callback.\n",
        "Early stoppingn helps our model from overfitting by stopping training if certain evaluation metrics is stopped.\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"
      ],
      "metadata": {
        "id": "k3n584swDMnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create early stopping callback.\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',\n",
        "                                                  patience =3)\n"
      ],
      "metadata": {
        "id": "deVgUqrt_Btj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training a model on a subset of data.\n",
        "Our 1st model is going to train only few images to make sure everything is working."
      ],
      "metadata": {
        "id": "GRJ1o9vTEkAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 100 #@param {type:\"slider\",min:10,max:100, step:10}\n"
      ],
      "metadata": {
        "id": "z_Wmj_1aEdNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPU is available\") if tf.config.list_physical_devices(\"GPU\") else print(\"Not Available\")"
      ],
      "metadata": {
        "id": "4zXyurY-Gc71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets create model which trains a model\n",
        "* Create a model using `create_model`\n",
        "* Setup a tensorboard callback using `create_tensorboard_callback()`\n",
        "* call the `fit()`on our model passing it to the training data,validation data,numer of epochs to train under `NUM_EPOCHS` and the callbacks we would like to use.\n",
        "* Return the model."
      ],
      "metadata": {
        "id": "joLwurbFHoea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create , build and train a model.\n",
        "def train_model():\n",
        "  \"\"\"\n",
        "    Trains a model and returns it\n",
        "  \"\"\"\n",
        "  #Create a model.\n",
        "  model = create_model()\n",
        "\n",
        "  # Create a new tensorboard session everytime we train a model.\n",
        "  tensorboard = create_tensorboard_callback()\n",
        "  #Fit the model to the data passing it to the callbacks we created.\n",
        "  model.fit(x = train_data,\n",
        "            epochs = NUM_EPOCHS,\n",
        "            validation_data = val_data,\n",
        "            validation_freq = 1,\n",
        "            callbacks = [tensorboard, early_stopping])\n",
        "  #Return the fitted model.\n",
        "  return model"
      ],
      "metadata": {
        "id": "KCCtiKaNHk4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model= train_model()"
      ],
      "metadata": {
        "id": "6OqdsEOiKuGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation dataset.\n",
        "predictions = model.predict(val_data, verbose =1)\n",
        "predictions"
      ],
      "metadata": {
        "id": "vpAwQMmoK5c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.shape"
      ],
      "metadata": {
        "id": "_BtI8BOXxBK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[2]"
      ],
      "metadata": {
        "id": "iJitBaLBxOti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(predictions[2])"
      ],
      "metadata": {
        "id": "Eiau-L2rxGVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first prediction\n",
        "index = 20\n",
        "print(predictions[index])\n",
        "print(f\"Max value ( Probability of predictions): {np.max(predictions[index])}\")\n",
        "print(f\"Sum:{np.sum(predictions[index])}\")\n",
        "print(f\"Max Index : {np.argmax(predictions[index])}\")\n",
        "print(f\"Predicted Label: {unique_labels[np.argmax(predictions[index])]}\")\n"
      ],
      "metadata": {
        "id": "kdozCzfzxMXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn prediction probabilities into their respective labels\n",
        "def get_pred_labels(prediction_probabilities):\n",
        "  \"\"\"\n",
        "  Turns an array of prediction probabilities into labels\n",
        "  \"\"\"\n",
        "  return unique_labels[np.argmax(prediction_probabilities)]"
      ],
      "metadata": {
        "id": "C-w_BScDyyXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a predicted label based on array of prediction probabilities\n",
        "pred_label = get_pred_labels(predictions[20])\n",
        "pred_label"
      ],
      "metadata": {
        "id": "uKY1ukFdzefo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data"
      ],
      "metadata": {
        "id": "5KHeTA3r1Ndw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation data set is in batches, we need to unbatch them to make predictions on validation data set."
      ],
      "metadata": {
        "id": "hsBdx97v1upp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to unbatch a batched Training and Validation dataset.\n",
        "def unbatchify(data):\n",
        "  images_ =[]\n",
        "  labels_ =[]\n",
        "  #Loop through the unbatched data.\n",
        "  for image, label in data.unbatch().as_numpy_iterator():\n",
        "    images_.append(image)\n",
        "    labels_.append(unique_labels[np.argmax(label)])\n",
        "  return images_, labels_"
      ],
      "metadata": {
        "id": "5qC6EdbK1T6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to unbatch a batched Test dataset.\n",
        "def unbatchify_test(data):\n",
        "  images_= []\n",
        "  for image in data.unbatch().as_numpy_iterator():\n",
        "    images_.append(image)\n",
        "  return images_\n"
      ],
      "metadata": {
        "id": "7TmKuivVD5XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unbatchify the validation dataset\n",
        "val_images, val_label = unbatchify(val_data)"
      ],
      "metadata": {
        "id": "LJwCql0F3eG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_images[0] , val_label[0]"
      ],
      "metadata": {
        "id": "FtFDfNpf3l-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pred_labels(val_label[0])"
      ],
      "metadata": {
        "id": "TlfkzNZy3sOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we have ways to get:\n",
        "* Prediction Labels\n",
        "* Validation Labels ( Truth Labels)\n",
        "* Validation Images\n",
        "Let's make a function to make them more visualizing, we will create a function which will do the following\n",
        "* Taken an array of prediction possibilities, array of truth label, an array of image and an integer.\n",
        "* Convert the prediction possibilities into the predicted label.\n",
        "* Plot the predicted label, its predicted possibilities, the truth label and the target image in a single plot.\n"
      ],
      "metadata": {
        "id": "UAovvThm5bn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pred(prediction_possibilities, labels, images, n=26):\n",
        "  \"\"\"\n",
        "  View predicted label, its predicted possibilities, the truth label and the target image in a single plot\n",
        "  \"\"\"\n",
        "  pred_prob, true_label, true_image = prediction_possibilities[n], labels[n], images[n]\n",
        "  # get the predicted label\n",
        "  pred_label = get_pred_labels(pred_prob)\n",
        "  # plot images and remove ticks\n",
        "  plt.imshow(true_image)\n",
        "  plt.xticks=[]\n",
        "  plt.yticks=[]\n",
        "  if pred_label == true_label:\n",
        "    color = \"green\"\n",
        "  else:\n",
        "    color =\"red\"\n",
        "  # Change plot title to get Predicted, Probability of prediction and the truth label\n",
        "  plt.title(\"{} {:2.0f}% {}\" .format(pred_label, np.max(pred_prob)*100, true_label),color=color)"
      ],
      "metadata": {
        "id": "FuGX4p8U31QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " plot_pred(predictions, labels = val_label, images = val_images, n=9)\n",
        ""
      ],
      "metadata": {
        "id": "rQimP5Y0Dbjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While we have got one more function to visualize our model's top prediction, let's make another view to visualize top 10 prediction of the model.\n",
        "This function will:\n",
        "* Take an input with the prediction probabilities array, Ground truth and an integer.\n",
        "* Find the prediction using `get_prep_labels()`\n",
        "* Find the top 10\n",
        "  * Prediction probabilities index\n",
        "  * Prection probabilities values\n",
        "  * Prediction labels\n",
        "* Plot top 10 prediction probabilty values and labels coloring true labels as `green`"
      ],
      "metadata": {
        "id": "f4TgOqn8kCoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_data)"
      ],
      "metadata": {
        "id": "x2dXY29hDpDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions on the test dataset."
      ],
      "metadata": {
        "id": "sWHzLwCrKxiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import shutil\n",
        "#data_dir = \"drive/MyDrive/Automobile/Test\"\n",
        "#shutil.rmtree(data_dir)"
      ],
      "metadata": {
        "id": "Vc74ZIiqaTO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if any file is not supported image type on test dataset.\n",
        "from pathlib import Path\n",
        "import imghdr\n",
        "\n",
        "data_dir = '../input/automobilepartsindentification/Automobile-parts\n",
        "image_extensions = [\".png\", \".jpg\"]  # add there all your images file extensions\n",
        "\n",
        "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
        "for filepath in Path(data_dir).rglob(\"*\"):\n",
        "    if filepath.suffix.lower() in image_extensions:\n",
        "        img_type = imghdr.what(filepath)\n",
        "        if img_type is None:\n",
        "            print(f\"{filepath} is not an image\")\n",
        "        elif img_type not in img_type_accepted_by_tf:\n",
        "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
      ],
      "metadata": {
        "id": "S_nFTDRgFhkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with full data.\n",
        "full_data = create_data_batches(X,y)\n",
        "full_data"
      ],
      "metadata": {
        "id": "XwYQBP-ZNNtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a model.\n",
        "full_model = create_model()"
      ],
      "metadata": {
        "id": "HsrenAEzcae3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_model.summary()"
      ],
      "metadata": {
        "id": "h2VP0PthdbFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS"
      ],
      "metadata": {
        "id": "KLkeRaduh1jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create full model callback\n",
        "full_model_tensorboard = create_tensorboard_callback()\n",
        "# No validation set when training on all the data , we cannot monitor accurary\n",
        "full_model_early_stopping = tf.keras.callbacks.EarlyStopping (monitor = \"accuracy\",\n",
        "                                                              patience =3)\n",
        "# Fit the full model\n",
        "full_model.fit(x= full_data,\n",
        "               epochs = NUM_EPOCHS,\n",
        "               callbacks = [full_model_tensorboard,full_model_early_stopping ])\n"
      ],
      "metadata": {
        "id": "81KXdDfbdd3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test image data files\n",
        "test_path = \"drive/MyDrive/Automobile/Test/\"\n",
        "test_filename = [test_path + fname for fname in os.listdir(test_path)]\n",
        "test_filename[:10]"
      ],
      "metadata": {
        "id": "YwIcqxEoek__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "if (os.path.exists(\"'../input/automobilepartsindentification/Automobile-parts/Test/.ipynb_checkpoints\")):\n",
        "  shutil.rmtree(\"'../input/automobilepartsindentification/Automobile-parts/Test/.ipynb_checkpoints\")"
      ],
      "metadata": {
        "id": "MCZ8Dvom7r9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import imghdr\n",
        "\n",
        "data_dir = \"'../input/automobilepartsindentification/Automobile-parts'Test/\"\n",
        "image_extensions = [\".png\", \".jpg\",\"jpeg\"]  # add there all your images file extensions\n",
        "\n",
        "img_type_accepted_by_tf = [\"bmp\", \"gif\", \"jpeg\", \"png\"]\n",
        "for filepath in Path(data_dir).rglob(\"*\"):\n",
        "    if filepath.suffix.lower() in image_extensions:\n",
        "        img_type = imghdr.what(filepath)\n",
        "        if img_type is None:\n",
        "            print(f\"{filepath} is not an image\")\n",
        "        elif img_type not in img_type_accepted_by_tf:\n",
        "            print(f\"{filepath} is a {img_type}, not accepted by TensorFlow\")"
      ],
      "metadata": {
        "id": "FEnCJQuTUphr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(test_path)))"
      ],
      "metadata": {
        "id": "wHMy7GkAjJo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test data batch\n",
        "test_data_batch = create_data_batches(test_filename, test_data = True)\n",
        "test_data_batch"
      ],
      "metadata": {
        "id": "Jw3KRqsFhQK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data batches\n",
        "test_prediction = full_model.predict(test_data_batch, verbose =1)"
      ],
      "metadata": {
        "id": "vWGctRyBgota"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction[0]"
      ],
      "metadata": {
        "id": "ENXVzdo_nU0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = unbatchify_test(test_data_batch)"
      ],
      "metadata": {
        "id": "QUbsvG-uat8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(prediction_test,images_test,end_batch=30):\n",
        "  \"\"\"\n",
        "  get the predicted image and predict its label.\n",
        "  \"\"\"\n",
        "  #Setup figure\n",
        "  plt.figure(figsize=(20,20))\n",
        "  for i in range(end_batch):\n",
        "    #print(i)\n",
        "    # plot images and remove ticks\n",
        "    ax = plt.subplot(10,10, i+1)\n",
        "    plt.imshow(test_image[i])\n",
        "    plt.xticks=[]\n",
        "    plt.yticks=[]\n",
        "    # Change plot title to get Predicted, Probability of prediction and the truth label\n",
        "    plt.title(\"{}\" .format(get_pred_labels(test_prediction[i])))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "ix53hHqig1p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_label(prediction_test = test_prediction, images_test = os.listdir(test_path), end_batch=100)"
      ],
      "metadata": {
        "id": "6yA1T3wewddy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}